# This file should contain all the record creation needed to seed the database with its default values.
# The data can then be loaded with the rake db:seed (or created alongside the db with db:setup).
#
# Examples:
#   
#   cities = City.create([{ :name => 'Chicago' }, { :name => 'Copenhagen' }])
#   Major.create(:name => 'Daley', :city => cities.first)
Researcher.create([{"salt"=>"c358de95ca39694aa1fb46ab2d6ae74ec4b8028e", "location"=>"United States", "remember_token_expires_at"=>Time.parse("Sat Jul 24 01:17:33 UTC 2010"), "last_login"=>Time.parse("Sat Jul 10 01:17:33 UTC 2010"), "crypted_password"=>"d0d8277fb66be6c1e99494add3c7e85f4c19591a", "join_date"=>Time.parse("Sat Jul 10 01:17:33 UTC 2010"), "website_url"=>nil, "role"=>"Admin", "last_access"=>Time.parse("Sat Jul 10 01:17:33 UTC 2010"), "info"=>nil, "id"=>356, "user_name"=>"Username", "remember_token"=>nil, "share_email"=>false, "reset_code"=>nil, "email"=>"admin@website.com"}])
News.create([{"slug"=>"welcome-to-140kit", "created_at"=>Time.parse("Thu Jul 01 02:59:16 UTC 2010"), "post"=>"<div class=\"newsItem col\">\r\n  <h1>Research</h1>\r\n  <p class=\"callout\">140kit is more than your personal stash of Tweets; when you <a href=\"/signup\">signup</a>, you have access to two powerful default scrape types: You can either search terms (with/without our similar term branching algorithm enabled) from this moment using the powerful <a href=\"http://dev.twitter.com/pages/streaming_api\" target=\"_blank\">Streaming API</a> or soon access one of our Whitelisted machines for REST access to collect as many tweets as possible from any number of accounts.</p>\r\n</div>\r\n<div class=\"newsItem col\">\r\n  <h1>Explore</h1>\r\n  <p class=\"callout\">Once your data <a href=\"/collections\">collection</a> is complete, you have access to an expanding list of <a href=\"analytical_offerings\">analytical offerings</a> to measure your data sets rapidly and in new ways. From there, you can quickly export data, view general charts, and soon have access to an experimental <a href=\"/networks/71/retweets\">re-tweet</a> network graph visualization. Use this data for <a href=\"http://journal.webscience.org/295/\">academic research</a>, one-off fact-checking <a href=\"http://opennet.net/blog/2010/06/iranelection-censored-evaluating-twitters-trending-topics\">blog posts</a>, or anything else you can think of, really.</p>\r\n</div>\r\n<div class=\"newsItem col\">\r\n  <h1>Collaborate</h1>\r\n  <p class=\"callout\">What if you wanted to <a href=\"/damxam/collections/37\">combine</a> <a href=\"/MaxD/collections/39\">multiple</a> data sets and look at their <a href=\"/Devin/collections/74\">sum value</a>? Doing that is simple with a \"Curation\" scrape, where you pick and choose existing data sets in the system, and mash them together for <a href=\"http://bit.ly/9jvKGZ\">epic win</a>. Don't see an analytical job you need? Develop it and throw it at our <a href=\"http://github.com/WebEcologyProject/140kit\">Git Account</a>, and, pending approval, it'll just be added in, with the proper attributions to your hard work.</p>\r\n</div>\r\n\r\n<div class=\"clearFloat\"></div>\r\n\r\n<h1>Go forth and research. Leave the gross parts to us.</h1>\r\n\r\n<br />\r\n\r\n<h2><a href=\"/pages/demo\">Still confused? Read the quick write-up on how to get around</a></h2>\r\n\r\n<h2><a href=\"/pages/upcoming-hackathon-general-business-junk-stuff-thanks\">Boston Hackathon, 1st Week Stats</a></h2>", "updated_at"=>Time.parse("Mon Jul 05 04:24:14 UTC 2010"), "raw_html"=>true, "headline"=>"Welcome to 140kit!", "page_item"=>true, "id"=>10, "researcher_id"=>1}])
AnalyticalOffering.create([
{"title"=>"Basic Histograms", "save_path"=>"/raw_data/graph_points/", "rest"=>false, "created_by_link"=>"http://www.devingaffney.com", "id"=>1, "enabled"=>true, "created_by"=>"Devin Gaffney", "function"=>"basic_histograms", "source_code_link"=>"http://github.com/WebEcologyProject/140kit/blob/master/back-end/cluster-code/analyzer/tools/basic_histograms.rb", "description"=>"Create basic histograms for all quantifiable attributes for users and tweets in the data set. Example: All users have follower counts (the number of people following them); this matches the number of followers to the number of users that have that many followers. With tweet's created_at time stamp, the number tweets created at every time step is created in a simple graph format. This data is then available in the collection view of your data set; you can then consume this information as google charts (with embed code included) data, JSON, CSV, or XML."}, 
{"title"=>"MySQL Dumper", "save_path"=>"/raw_data/raw_sql/", "rest"=>false, "created_by_link"=>"http://www.devingaffney.com", "id"=>2, "enabled"=>true, "created_by"=>"Devin Gaffney", "function"=>"mysql_dumper", "source_code_link"=>"http://github.com/WebEcologyProject/140kit/blob/master/back-end/cluster-code/analyzer/tools/mysql_dumper.rb", "description"=>"Dump all the data from your collection into .sql files, which are then available on our file system on the server for ever and ever. It's always a bad idea to have no back-ups of data; it's even worse to not have redundant data. This allows you to create your own MySQL database with all your tweets and users intact (and mapped to one another with our indices) without losing a beat so that you can do whatever you will with the data separately and off of our site in case we don't have the analytics you want."}, 
{"title"=>"Raw CSV", "save_path"=>"/raw_data/raw_csv/", "rest"=>false, "created_by_link"=>"http://www.devingaffney.com", "id"=>3, "enabled"=>true, "created_by"=>"Devin Gaffney", "function"=>"raw_csv", "source_code_link"=>"http://github.com/WebEcologyProject/140kit/blob/master/back-end/cluster-code/analyzer/tools/raw_csv.rb", "description"=>"Dump all the data from your collection into .csv files, which are then available on our file system on the server for ever and ever. With CSV, you can easily open up the data in Excel, OpenOffice (god bless you), or something similar, or just push it into some other data base solution as this is one of the most simple forms of transitioning data from one place to another. Allows for simple access to the data without much thought put into it, and gets you rolling on your own research."}, 
{"title"=>"Retweet Graph", "save_path"=>"/raw_data/retweet_graphs/", "rest"=>true, "created_by_link"=>"http://www.ianpearce.info", "id"=>4, "enabled"=>false, "created_by"=>"Ian Pearce", "function"=>"retweet_graph", "source_code_link"=>"http://github.com/WebEcologyProject/140kit/blob/master/back-end/cluster-code/analyzer/tools/retweet_graph.rb", "description"=>"This one is a bit hard to explain, but I'll try my best. For any set of tweets, there are some that are \"conversational,\" or are in other words \"direct mentions\" or \"retweets.\" If you grab all of these tweets, then map out who was saying what and to whom, then you can draw a network map of the different interactions in the dataset between the users; User A retweeted User B, and User B in turn retweeted User C; User A, then, is one degree of separation away in interacting with User C; and User C is the most influential in this chain. If you do this with everyone, you get really neat graphs that can help you quickly identify who is important in the network. This data is given to you in our own on-site visualization, but is also consumable via JSON and GraphML (basically just a highly formatted XML that is popular with network geeks)."}])